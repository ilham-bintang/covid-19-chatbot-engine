{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ilhambintang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#@title Setup common imports and functions\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import simpleneighbors\n",
    "import urllib\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def download_squad(url):\n",
    "  return json.load(urllib.request.urlopen(url))\n",
    "\n",
    "def extract_sentences_from_squad_json(squad):\n",
    "  all_sentences = []\n",
    "  for data in squad['data']:\n",
    "    for paragraph in data['paragraphs']:\n",
    "      sentences = nltk.tokenize.sent_tokenize(paragraph['context'])\n",
    "      all_sentences.extend(zip(sentences, [paragraph['context']] * len(sentences)))\n",
    "  return list(set(all_sentences)) # remove duplicates\n",
    "\n",
    "def extract_questions_from_squad_json(squad):\n",
    "  questions = []\n",
    "  for data in squad['data']:\n",
    "    for paragraph in data['paragraphs']:\n",
    "      for qas in paragraph['qas']:\n",
    "        if qas['answers']:\n",
    "          questions.append((qas['question'], qas['answers'][0]['text']))\n",
    "  return list(set(questions))\n",
    "\n",
    "def output_with_highlight(text, highlight):\n",
    "  output = \"<li> \"\n",
    "  i = text.find(highlight)\n",
    "  while True:\n",
    "    if i == -1:\n",
    "      output += text\n",
    "      break\n",
    "    output += text[0:i]\n",
    "    output += '<b>'+text[i:i+len(highlight)]+'</b>'\n",
    "    text = text[i+len(highlight):]\n",
    "    i = text.find(highlight)\n",
    "  return output + \"</li>\\n\"\n",
    "\n",
    "def display_nearest_neighbors(query_text, answer_text=None):\n",
    "  query_embedding = model.signatures['question_encoder'](tf.constant([query_text]))['outputs'][0]\n",
    "  search_results = index.nearest(query_embedding, n=num_results)\n",
    "\n",
    "  if answer_text:\n",
    "    result_md = '''\n",
    "    <p>Random Question from SQuAD:</p>\n",
    "    <p>&nbsp;&nbsp;<b>%s</b></p>\n",
    "    <p>Answer:</p>\n",
    "    <p>&nbsp;&nbsp;<b>%s</b></p>\n",
    "    ''' % (query_text , answer_text)\n",
    "  else:\n",
    "    result_md = '''\n",
    "    <p>Question:</p>\n",
    "    <p>&nbsp;&nbsp;<b>%s</b></p>\n",
    "    ''' % query_text\n",
    "\n",
    "  result_md += '''\n",
    "    <p>Retrieved sentences :\n",
    "    <ol>\n",
    "  '''\n",
    "\n",
    "  if answer_text:\n",
    "    for s in search_results:\n",
    "      result_md += output_with_highlight(s, answer_text)\n",
    "  else:\n",
    "    for s in search_results:\n",
    "      result_md += '<li>' + s + '</li>\\n'\n",
    "\n",
    "  result_md += \"</ol>\"\n",
    "  display(HTML(result_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10455 sentences, 10552 questions extracted from SQuAD https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
      "\n",
      "Example sentence and context:\n",
      "\n",
      "sentence:\n",
      "\n",
      "('On May 28, 2012, Jacksonville was hit by Tropical Storm Beryl, packing winds '\n",
      " 'up to 70 miles per hour (113 km/h) which made landfall near Jacksonville '\n",
      " 'Beach.')\n",
      "\n",
      "context:\n",
      "\n",
      "('Jacksonville has suffered less damage from hurricanes than most other east '\n",
      " 'coast cities, although the threat does exist for a direct hit by a major '\n",
      " 'hurricane. The city has only received one direct hit from a hurricane since '\n",
      " '1871; however, Jacksonville has experienced hurricane or near-hurricane '\n",
      " 'conditions more than a dozen times due to storms crossing the state from the '\n",
      " 'Gulf of Mexico to the Atlantic Ocean, or passing to the north or south in '\n",
      " 'the Atlantic and brushing past the area. The strongest effect on '\n",
      " 'Jacksonville was from Hurricane Dora in 1964, the only recorded storm to hit '\n",
      " 'the First Coast with sustained hurricane-force winds. The eye crossed St. '\n",
      " 'Augustine with winds that had just barely diminished to 110 mph (180 km/h), '\n",
      " 'making it a strong Category 2 on the Saffir-Simpson Scale. Jacksonville also '\n",
      " \"suffered damage from 2008's Tropical Storm Fay which crisscrossed the state, \"\n",
      " 'bringing parts of Jacksonville under darkness for four days. Similarly, four '\n",
      " 'years prior to this, Jacksonville was inundated by Hurricane Frances and '\n",
      " 'Hurricane Jeanne, which made landfall south of the area. These tropical '\n",
      " 'cyclones were the costliest indirect hits to Jacksonville. Hurricane Floyd '\n",
      " 'in 1999 caused damage mainly to Jacksonville Beach. During Floyd, the '\n",
      " 'Jacksonville Beach pier was severely damaged, and later demolished. The '\n",
      " 'rebuilt pier was later damaged by Fay, but not destroyed. Tropical Storm '\n",
      " 'Bonnie would cause minor damage in 2004, spawning a minor tornado in the '\n",
      " 'process. On May 28, 2012, Jacksonville was hit by Tropical Storm Beryl, '\n",
      " 'packing winds up to 70 miles per hour (113 km/h) which made landfall near '\n",
      " 'Jacksonville Beach.')\n",
      "\n",
      "CPU times: user 617 ms, sys: 40 ms, total: 657 ms\n",
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "squad_url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json' #@param [\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"]\n",
    "\n",
    "squad_json = download_squad(squad_url)\n",
    "sentences = extract_sentences_from_squad_json(squad_json)\n",
    "questions = extract_questions_from_squad_json(squad_json)\n",
    "print(\"%s sentences, %s questions extracted from SQuAD %s\" % (len(sentences), len(questions), squad_url))\n",
    "\n",
    "print(\"\\nExample sentence and context:\\n\")\n",
    "sentence = random.choice(sentences)\n",
    "print(\"sentence:\\n\")\n",
    "pprint.pprint(sentence[0])\n",
    "print(\"\\ncontext:\\n\")\n",
    "pprint.pprint(sentence[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 s, sys: 2.87 s, total: 30.7 s\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-qa/3\"\n",
    "model = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 10455 sentences\n",
      "simpleneighbors index for 10455 sentences built.\n",
      "CPU times: user 14min 42s, sys: 1min 5s, total: 15min 47s\n",
      "Wall time: 12min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "encodings = model.signatures['response_encoder'](\n",
    "  input=tf.constant([sentences[0][0]]),\n",
    "  context=tf.constant([sentences[0][1]]))\n",
    "index = simpleneighbors.SimpleNeighbors(\n",
    "    len(encodings['outputs'][0]), metric='angular')\n",
    "\n",
    "print('Computing embeddings for %s sentences' % len(sentences))\n",
    "slices = zip(*(iter(sentences),) * batch_size)\n",
    "num_batches = int(len(sentences) / batch_size)\n",
    "for n, s in enumerate(slices):\n",
    "  response_batch = list([r for r, c in s])\n",
    "  context_batch = list([c for r, c in s])\n",
    "  encodings = model.signatures['response_encoder'](\n",
    "    input=tf.constant(response_batch),\n",
    "    context=tf.constant(context_batch)\n",
    "  )\n",
    "  for i in range(len(response_batch)):\n",
    "    index.add_one(response_batch[i], encodings['outputs'][i])\n",
    "\n",
    "index.build()\n",
    "print('simpleneighbors index for %s sentences built.' % len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_results = 25\n",
    "\n",
    "query = random.choice(questions)\n",
    "display_nearest_neighbors(query[0], query[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
